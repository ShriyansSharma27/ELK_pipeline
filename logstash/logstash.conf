# Kafka input configuration 
input {
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["logs"] 
    group_id => "logstash-group"
    auto_offset_reset => "earliest"  
    codec => "json"
    session_timeout_ms => 30000
  }
}

# Filter is used to parse JSON and grok is used to handle random messy string logs
filter {

  # If the message is not of an appropriate type
  if !([log_type] in ["app", "auth", "payment", "sql"]) {
    mutate {
      add_tag => ["unparsed"]
    }
  }

  # If the message is of an appropriate type
  else {
    if [log_type] == "app" {
      grok{
        match => {
          "message" => "%{LOGLEVEL:level} %{MONTHDAY:rec_day}-%{MONTHNUM:rec_month}-%{YEAR:rec_year} %{TIME:stmp} app %{GREEDYDATA:app_msg}"
        }
      }
    }

    else if [log_type] == "auth" {
      grok{
        match => {
          "message" => "%{LOGLEVEL:level} %{MONTHDAY:rec_day}-%{MONTHNUM:rec_month}-%{YEAR:rec_year} %{TIME:stmp} auth %{GREEDYDATA:auth_msg}"
        }
      }
    }


    else if [log_type] == "payment" {
      grok{
        match => {
          "message" => "%{LOGLEVEL:level} %{MONTHDAY:rec_day}-%{MONTHNUM:rec_month}-%{YEAR:rec_year} %{TIME:stmp} payment %{GREEDYDATA:payment_msg}"
        }
      }
    }

    else if [log_type] == "sql" {
      grok{
        match => {
          "message" => "%{LOGLEVEL:level} %{MONTHDAY:rec_day}-%{MONTHNUM:rec_month}-%{YEAR:rec_year} %{TIME:stmp} sql %{GREEDYDATA:sql_exec}"
        }
      }
    }

    # Repeated error/critical messages after a count of 20 is flagged
    if [level] == "ERROR" or [level] == "CRITICAL" {
      throttle {
        key => "%{log_type}-%{level}"
        period => 180
        after_count => 20
        add_tag => ["recurring"]
      }
      mutate { add_tag => ["send-alert"] }
    }

    # Date is stored and standardized from the messy log string
    if [rec_day] and [rec_month] and [rec_year] and [stmp] {
      mutate {
        add_field => {"complete_date" => "%{rec_day}-%{rec_month}-%{rec_year} %{stmp}"}
      }

      date {
        match => ["complete_date", "dd-MM-yyyy HH:mm:ss", "dd-M-yyyy HH:mm:ss"]
        target => "@timestamp"
        timezone => "America/Toronto"
        remove_field => ["complete_date"]
      }
    }
  }
}

output {
  # Destination for messages that were not processed correctly
  if "unparsed" in [tags] {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "logs-deadletter"
    }
  }

  # Destination for messages that were correct processed
  else {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "logs"
    }
  }

  # Monitoring of critical and error logs
  if "send-alert" in [tags] {
    http {
      url => "https://${BETTER_STACK_URL}"
      http_method => "post"
      headers => { "Authorization" => "Bearer ${BETTER_STACK_TOKEN}" }
      format => "json"
    }
  }
}
